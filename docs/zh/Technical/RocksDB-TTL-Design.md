# RocksDB 数据过期删除（TTL）设计方案

> 本文档重点讲解设计思路、架构理念和方案对比，不涉及具体代码实现

---

## 一、问题背景与挑战

### 1.1 为什么需要数据过期？

**业务需求：**
- **存储成本控制：** 历史数据无限增长会导致存储成本持续上升
- **性能保证：** 数据量过大影响读写性能和查询效率
- **合规要求：** 某些业务数据需要在一定时间后自动清理（如GDPR）
- **业务逻辑：** 缓存、临时数据、会话信息等天然有时效性

**核心挑战：**
- 如何在不影响正常读写性能的前提下清理数据？
- 如何保证过期时间的精确性？
- 如何平衡删除及时性和系统开销？
- 如何处理索引一致性问题？

### 1.2 RobustMQ 的场景特点

**数据特征：**
- 消息数据量大，写入频繁
- 有多种索引（key索引、tag索引、时间戳索引）
- 不同namespace/shard可能需要不同的保留策略
- 需要保证索引与主数据的一致性

**性能要求：**
- 不能影响正常的消息读写性能
- 删除操作不能阻塞业务
- 需要及时回收存储空间

---

## 二、业界主流解决方案对比

### 2.1 方案一：RocksDB 内置 TTL

**核心思路：**
- RocksDB 提供原生的 `DBWithTTL` API
- 在打开数据库时指定统一的TTL时长
- 底层通过 CompactionFilter 自动清理过期数据

**设计理念：**
- **零侵入：** 应用层无需关心过期逻辑
- **自动化：** 与Compaction流程深度集成
- **统一管理：** 所有数据使用相同的过期策略

**优点：**
- ✅ 实现极其简单，开箱即用
- ✅ RocksDB原生支持，性能和稳定性有保证
- ✅ 不需要额外的后台线程或定时任务
- ✅ 与LSM-Tree的Compaction天然契合

**缺点：**
- ❌ 灵活性差：所有数据必须使用相同的TTL
- ❌ 粒度粗：只能在数据库或列族级别设置
- ❌ 无法满足"不同消息不同保留时间"的需求
- ❌ 删除依赖Compaction触发，不够及时

**适用场景：**
- 缓存系统（所有缓存条目生命周期一致）
- 临时数据存储
- Session管理（统一的会话超时时间）

**不适用场景：**
- 需要按消息/记录级别设置不同TTL
- 需要及时删除的场景
- 有复杂索引需要同步删除的场景（RobustMQ的情况）

---

### 2.2 方案二：应用层时间戳 + 定期扫描

**核心思路：**
- 在每条记录中存储过期时间戳
- 启动后台线程定期全表扫描
- 发现过期数据后批量删除

**设计理念：**
- **完全控制：** 应用层掌握所有逻辑，便于调试
- **灵活配置：** 每条记录可以有不同的TTL
- **主动清理：** 不依赖RocksDB的Compaction

**优点：**
- ✅ 灵活性最强，可为每条记录设置不同TTL
- ✅ 实现相对简单，逻辑清晰
- ✅ 可以精确控制删除时机和频率
- ✅ 可以同步删除索引，保证一致性

**缺点：**
- ❌ 全表扫描性能开销大
- ❌ 扫描期间会消耗大量CPU和IO
- ❌ 过期数据会占用存储空间直到下次扫描
- ❌ 需要权衡扫描频率和系统负载
- ❌ 扫描频率低导致及时性差，频率高导致性能差

**优化思路：**
- 分批扫描，避免一次扫描所有数据
- 使用限流机制，控制删除速度
- 在低峰期执行扫描任务
- 设置单次扫描的最大数据量

**适用场景：**
- 数据量不是特别大的场景
- 对删除及时性要求不高
- 需要灵活的TTL策略

---

### 2.3 方案三：CompactionFilter（RocksDB推荐）

**核心思路：**
- 实现自定义的 CompactionFilter
- 在Compaction过程中判断数据是否过期
- 过期数据直接丢弃，不写入新的SST文件

**设计理念：**
- **顺势而为：** 利用RocksDB的Compaction机制
- **无额外开销：** 不需要单独的扫描线程
- **自然清理：** 过期数据在Compaction时自动消失

**工作原理：**
```
正常Compaction流程：
  SST-1 + SST-2 -> 读取 -> 合并排序 -> 写入新SST

加入CompactionFilter：
  SST-1 + SST-2 -> 读取 -> 合并排序 
    -> 每条数据调用filter判断 
    -> 过期数据丢弃 
    -> 只写入有效数据到新SST
```

**优点：**
- ✅ 性能最优，零额外开销
- ✅ 不影响正常读写
- ✅ 自动与RocksDB的后台任务集成
- ✅ 可以为每条记录设置不同TTL
- ✅ 彻底清理数据，释放存储空间

**缺点：**
- ❌ 删除不及时，依赖Compaction触发
- ❌ 在Compaction之前，过期数据仍占用空间
- ❌ 无法控制删除的精确时机
- ❌ 难以同步删除索引（Compaction只看到当前记录）

**重要考虑：**
- Compaction触发时机不可控（可能几小时甚至几天）
- 如果写入量小，Compaction触发少，过期数据长期存在
- 可以手动触发Compaction，但代价较大

**适用场景：**
- 对删除及时性要求不高
- 写入量大，Compaction频繁
- 追求零额外性能开销
- 单纯的key-value存储（无复杂索引）

---

### 2.4 方案四：懒删除（Lazy Deletion）

**核心思路：**
- 数据写入时记录过期时间
- 读取时检查是否过期
- 如果过期，返回"不存在"并异步删除

**设计理念：**
- **延迟处理：** 只在需要时才处理过期
- **用户无感：** 用户读不到过期数据
- **分散开销：** 删除开销分散到每次读操作

**工作流程：**
```
1. 写入数据时：记录 expire_at 时间戳
2. 读取数据时：
   - 读取到数据
   - 检查 expire_at < now ?
   - 如果过期：
     * 返回 None（用户感知为不存在）
     * 标记删除（加入删除队列）
   - 如果未过期：返回数据
3. 后台线程：批量处理删除队列
```

**优点：**
- ✅ 用户体验最好，过期立即生效
- ✅ 不需要定期扫描
- ✅ 删除开销均匀分布
- ✅ 实现相对简单

**缺点：**
- ❌ 每次读取都要检查时间戳，有开销
- ❌ 如果数据从不被读取，永远不会删除
- ❌ "冷数据"会一直占用空间
- ❌ 需要异步删除队列，增加复杂度

**优化思路：**
- 结合定期扫描，清理冷数据
- 异步删除队列批量提交
- 使用BloomFilter加速过期判断

**适用场景：**
- 缓存系统（热数据为主）
- 对用户体验要求高
- 数据访问频繁的场景

---

### 2.5 方案五：TTL索引优化

**核心思路：**
- 单独维护一个"过期时间索引"
- Key格式：`/ttl/{expire_timestamp}/{record_id}`
- 扫描时按时间顺序查找，效率更高

**设计理念：**
- **索引加速：** 避免全表扫描
- **有序存储：** 按过期时间排序
- **提前终止：** 扫描到未过期数据即可停止

**数据组织：**
```
主数据：
  /record/namespace1/shard1/0 -> Record{data, timestamp}
  /record/namespace1/shard1/1 -> Record{data, timestamp}

TTL索引：
  /ttl/1638000000/namespace1/shard1/0 -> empty
  /ttl/1638000100/namespace1/shard1/1 -> empty
  /ttl/1638000200/namespace2/shard2/5 -> empty
```

**扫描流程：**
```
1. 获取当前时间：now = 1638000150
2. 从 /ttl/ 开始扫描
3. 读到 /ttl/1638000000/... -> 过期，删除
4. 读到 /ttl/1638000100/... -> 过期，删除  
5. 读到 /ttl/1638000200/... -> 未过期，停止扫描！
6. 后面的数据一定也未过期（有序）
```

**优点：**
- ✅ 扫描效率极高，不需要遍历所有数据
- ✅ 可以提前终止，避免无效扫描
- ✅ 删除操作更加精准
- ✅ 适合大数据量场景

**缺点：**
- ❌ 需要维护额外的索引
- ❌ 写入时需要双写（主数据 + TTL索引）
- ❌ 占用额外存储空间
- ❌ 需要保证索引一致性

**空间成本分析：**
- 每条记录额外一个TTL索引
- 索引key长度约50-100字节
- 如果有1亿条记录，额外占用5-10GB

**适用场景：**
- 数据量特别大（百万级以上）
- 定期扫描性能瓶颈明显
- 可以接受额外存储开销

---

## 三、业界实际案例

### 3.1 Redis 的过期策略

**组合策略：定期删除 + 惰性删除**

1. **定期删除（Periodic Deletion）：**
   - 每秒执行10次随机抽样
   - 每次随机抽取20个key检查
   - 如果过期超过25%，继续抽样
   - 单次执行时间不超过25ms

2. **惰性删除（Lazy Deletion）：**
   - 访问key时检查是否过期
   - 过期则删除并返回不存在

3. **内存淘汰（Memory Eviction）：**
   - 内存不足时主动清理
   - 多种淘汰算法（LRU、LFU等）

**设计亮点：**
- 随机抽样，避免全表扫描
- 自适应调整，过期多则多扫描
- 时间限制，避免阻塞主线程
- 多层防护，确保内存可控

---

### 3.2 Kafka 的日志清理

**基于段（Segment）的清理策略**

1. **时间保留（Time-based Retention）：**
   - 按Segment为单位删除
   - 检查Segment的最后修改时间
   - 整个Segment过期则直接删除文件

2. **大小保留（Size-based Retention）：**
   - 限制Topic的总大小
   - 从最旧的Segment开始删除

3. **日志压缩（Log Compaction）：**
   - 只保留每个key的最新值
   - 类似于CompactionFilter的思路

**设计亮点：**
- 以Segment为单位，避免细粒度删除
- 不需要扫描每条消息
- 直接删除文件，速度快
- 多种策略可组合使用

---

### 3.3 Cassandra 的TTL机制

**墓碑（Tombstone）机制**

1. **写入时指定TTL：**
   - 每条记录可以有不同的TTL
   - 过期时间记录在数据中

2. **读取时判断：**
   - 读到过期数据时跳过
   - 不立即删除，只标记

3. **Compaction时清理：**
   - 真正删除在Compaction时进行
   - 使用墓碑标记已删除数据

**设计亮点：**
- 灵活的per-record TTL
- 分布式友好（墓碑可以传播）
- 懒删除 + Compaction清理的组合

---

### 3.4 LevelDB/RocksDB 官方建议

**官方推荐方案：**

1. **优先使用CompactionFilter**
   - 性能最优
   - 与LSM-Tree架构契合

2. **结合应用层检查**
   - 读取时判断是否过期
   - 提升用户体验

3. **避免全表扫描**
   - 使用前缀索引
   - 范围查询代替全表扫描

4. **手动触发Compaction**
   - 大量删除后手动compact
   - 及时回收空间

---

## 四、RobustMQ 的最佳实践方案

### 4.1 现状分析

**当前实现（基于代码分析）：**
- 采用"应用层时间戳 + 定期扫描"方案
- 后台线程定期遍历所有shard
- 使用WriteBatch批量删除
- 同时删除主记录和索引（key、tag、timestamp）

**存在的问题：**
- 全表扫描性能开销大
- 扫描间隔内的过期数据仍可被读取
- 大量数据时扫描时间长
- 没有空间回收的保证（依赖RocksDB自动Compaction）

### 4.2 推荐的三层过期机制

```
┌─────────────────────────────────────────────┐
│         第一层：读时检查（Lazy）              │
│  - 读取时立即判断                            │
│  - 用户体验最好                              │
│  - 异步标记删除                              │
│  优先级：高 | 及时性：立即 | 开销：低         │
└─────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────┐
│       第二层：TTL索引扫描（Periodic）         │
│  - 基于TTL索引，快速定位过期数据              │
│  - 批量删除，性能可控                        │
│  - 清理冷数据                                │
│  优先级：中 | 及时性：分钟级 | 开销：中       │
└─────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────┐
│    第三层：CompactionFilter（Background）    │
│  - Compaction时自动清理                      │
│  - 彻底回收存储空间                          │
│  - 兜底保证                                  │
│  优先级：低 | 及时性：小时级 | 开销：零       │
└─────────────────────────────────────────────┘
```

### 4.3 方案设计思路

#### 第一层：读时检查（用户体验）

**设计目标：**
- 用户读不到过期数据
- 不阻塞读操作
- 分散删除压力

**实现思路：**
1. 读取时检查时间戳
2. 如果过期：
   - 立即返回None（用户无感知）
   - 将删除任务放入队列（异步）
3. 后台worker批量处理删除队列

**关键点：**
- 读路径上只做判断，不做删除
- 删除操作异步化，不阻塞
- 队列满时可以丢弃（反正有第二层兜底）

---

#### 第二层：TTL索引扫描（主动清理）

**设计目标：**
- 高效定位过期数据
- 清理"冷数据"（从不被访问的数据）
- 可控的性能开销

**数据结构设计：**
```
主记录：
  /record/{namespace}/{shard}/{offset} -> Record

TTL索引（新增）：
  /ttl/{expire_timestamp_padded}/{namespace}/{shard}/{offset} -> empty
  
说明：
  - expire_timestamp_padded：过期时间戳，左补零到固定长度
  - 按时间有序排列
  - value可以为空，节省空间
```

**扫描流程：**
1. 获取当前时间戳
2. 从 `/ttl/` 开始seek
3. 顺序读取，检查时间戳
4. 如果过期：加入删除批次
5. 如果未过期：停止扫描（后面的一定也未过期）
6. 批量删除主记录和所有索引

**性能优化：**
- 分批提交（如每1000条）
- 单次扫描限制总数（如10万条）
- 扫描间隔可配置（如每小时）
- 在业务低峰期执行

**空间成本：**
- 每条记录约增加 50-80 字节
- 1亿条记录约增加 5-8GB
- 可接受的代价

---

#### 第三层：CompactionFilter（空间回收）

**设计目标：**
- 彻底清理过期数据
- 释放存储空间
- 零额外性能开销

**实现思路：**
1. 实现自定义CompactionFilter
2. 只过滤主记录（/record/ 前缀）
3. 判断时间戳是否过期
4. 过期则返回Remove决策

**注意事项：**
- 不要在Filter中处理索引（只看到单条记录）
- 索引由第一、二层负责清理
- 作为兜底机制，确保数据最终被清理

**触发时机：**
- 自动：RocksDB后台自动触发
- 手动：大量删除后可手动触发（compact_range）

---

### 4.4 索引一致性保证

**挑战：**
RobustMQ有多种索引：
- Key索引：`/key/{namespace}/{shard}/{key}` -> offset
- Tag索引：`/tag/{namespace}/{shard}/{tag}/{offset}` -> empty
- Timestamp索引：`/timestamp/{namespace}/{shard}/{ts}/{offset}` -> empty

必须保证主记录和索引同步删除，否则：
- 孤儿索引：索引存在但记录不存在（查询失败）
- 脏数据：记录存在但索引不存在（查询遗漏）

**解决方案：**

1. **WriteBatch原子性：**
   - 主记录和所有索引在同一个WriteBatch
   - 要么全部成功，要么全部失败
   - RocksDB保证原子性

2. **删除顺序：**
   ```
   正确顺序：
   1. 先删除所有索引
   2. 再删除主记录
   
   原因：
   - 如果主记录先删除，查询可能通过索引找到offset
   - 但读取主记录时发现不存在（中间状态）
   - 先删索引，最多查不到，不会出错
   ```

3. **Key索引特殊处理：**
   ```
   问题：
   - Key索引是 key -> latest_offset 的映射
   - 如果同一个key有多条记录，删除旧记录时不能删索引
   
   解决：
   - 删除前先读取Key索引
   - 判断索引指向的offset是否是当前要删除的offset
   - 只有相等时才删除索引
   ```

4. **CompactionFilter的限制：**
   - Filter只看到单条记录，无法处理索引
   - 索引的清理完全由第一、二层负责
   - Filter只负责主记录，作为兜底

---

### 4.5 配置化设计

**配置项设计思路：**

```yaml
ttl:
  # 全局默认配置
  default_ttl_seconds: 604800  # 7天
  
  # 扫描配置
  scan:
    enabled: true
    interval_seconds: 3600      # 每小时扫描
    max_delete_per_scan: 100000 # 单次最多删除10万条
    batch_size: 1000            # 每批提交1000条
  
  # 读时检查
  read_time_check:
    enabled: true
    async_delete_queue_size: 10000
  
  # CompactionFilter
  compaction_filter:
    enabled: true
  
  # 按namespace自定义TTL
  namespace_ttl:
    "mqtt-messages": 86400      # MQTT消息保留1天
    "system-logs": 2592000      # 系统日志保留30天
    "temp-data": 3600           # 临时数据保留1小时
```

**设计原则：**
- 灵活配置，适应不同场景
- 可动态调整（热更新）
- 分层开关，可独立启用/禁用
- 细粒度控制（namespace级别）

---

## 五、性能与权衡

### 5.1 及时性 vs 性能

**及时性排序（从高到低）：**
1. 读时检查：立即生效（毫秒级）
2. TTL索引扫描：分钟到小时级
3. CompactionFilter：小时到天级

**性能开销排序（从低到高）：**
1. CompactionFilter：零额外开销
2. 读时检查：每次读取增加一次时间戳比较（纳秒级）
3. TTL索引扫描：可控的后台开销

**推荐配置：**
- 对及时性要求高：启用读时检查 + TTL索引
- 对性能敏感：只用CompactionFilter + 低频扫描
- 生产环境：三层全开（最佳平衡）

---

### 5.2 空间 vs 效率

**空间成本：**
- 无TTL索引：0额外空间，全表扫描
- 有TTL索引：5-10%额外空间，高效扫描

**扫描效率对比：**
```
无TTL索引（全表扫描）：
  - 需要遍历所有记录
  - 1亿条记录，扫描时间可能10-30分钟
  - 大量无效IO

有TTL索引（有序扫描）：
  - 只扫描过期部分
  - 1亿条记录，假设1%过期，扫描时间<1分钟
  - IO集中在有效数据
```

**建议：**
- 数据量<1000万：无需TTL索引
- 数据量>1000万：强烈建议TTL索引
- 数据量>1亿：TTL索引必须

---

### 5.3 一致性 vs 复杂度

**一致性要求：**
- 强一致性：WriteBatch原子删除
- 最终一致性：允许短暂的索引-数据不一致

**复杂度来源：**
- 多索引同步删除
- Key索引的特殊判断
- 异步删除队列管理

**简化建议：**
- 如果可以接受索引不一致（如统计允许误差）
- 可以简化删除逻辑，不删除索引
- 依靠CompactionFilter最终清理

---

## 六、监控与可观测性

### 6.1 关键指标

**业务指标：**
- 过期数据占比（过期记录数 / 总记录数）
- 平均数据存活时间
- 不同namespace的过期数据分布

**性能指标：**
- 扫描耗时（每次扫描的持续时间）
- 删除速率（每秒删除记录数）
- 扫描期间的读写延迟变化

**资源指标：**
- 存储空间变化趋势
- Compaction次数和IO
- 删除队列长度（读时检查）

### 6.2 告警策略

**需要告警的情况：**
- 过期数据占比>20%（扫描不及时）
- 扫描耗时>10分钟（数据量太大或性能差）
- 删除队列积压>阈值（异步删除跟不上）
- 存储空间持续增长（删除未生效）

---

## 七、实施路线图

### 阶段0：现状优化（短期，1-2周）
**目标：在现有架构上优化**
- 优化扫描批次大小和频率
- 添加监控指标
- 配置化TTL参数
- 压力测试，找到最佳参数

**收益：**
- 立即可见的性能提升
- 了解系统瓶颈
- 为后续优化提供基准

---

### 阶段1：引入TTL索引（中期，2-3周）
**目标：解决扫描性能瓶颈**
- 设计TTL索引格式
- 修改写入逻辑，双写TTL索引
- 实现基于TTL索引的扫描
- 数据迁移方案（为已有数据补充TTL索引）

**收益：**
- 扫描效率提升10-100倍（取决于数据量）
- 支持更高频率的扫描
- 为大规模数据做好准备

---

### 阶段2：读时检查（中期，1-2周）
**目标：提升用户体验**
- 实现读时TTL检查
- 实现异步删除队列
- 压测验证性能影响

**收益：**
- 用户读不到过期数据
- 删除压力更加均匀
- 热数据及时清理

---

### 阶段3：CompactionFilter（长期，2-3周）
**目标：彻底空间回收**
- 实现TTL CompactionFilter
- 集成到RocksDB配置
- 验证空间回收效果

**收益：**
- 确保数据最终被清理
- 回收存储空间
- 系统更加健壮

---

### 阶段4：精细化运营（持续）
**目标：持续优化**
- 根据监控数据调优
- 不同业务场景的TTL策略
- 自适应的扫描频率
- 自动化的空间管理

---

## 八、总结与建议

### 核心设计原则

1. **分层防护：** 读时检查 + 定期扫描 + Compaction兜底
2. **按需优化：** 数据量小时简单方案，数据量大时引入索引
3. **渐进式演进：** 先优化现有方案，再逐步引入新机制
4. **可观测性：** 完善的监控和告警，数据驱动优化
5. **权衡取舍：** 明确每个方案的优缺点，根据场景选择

### RobustMQ 的推荐方案

**短期（立即实施）：**
- 优化现有扫描逻辑（批次控制、限流）
- 完善监控指标

**中期（1-2个月）：**
- 引入TTL索引（核心优化）
- 实现读时检查

**长期（3-6个月）：**
- 实现CompactionFilter
- 精细化运营

### 关键成功因素

1. **充分测试：**
   - 功能测试：验证过期逻辑正确性
   - 性能测试：验证不影响正常业务
   - 压力测试：验证极端情况下的表现

2. **灰度发布：**
   - 先在小流量namespace测试
   - 逐步扩大范围
   - 随时可回滚

3. **持续监控：**
   - 关注关键指标
   - 及时发现问题
   - 数据驱动决策

4. **文档完善：**
   - 设计文档
   - 配置说明
   - 运维手册

---

## 附录：业界对比总结

| 系统 | 主要策略 | 及时性 | 复杂度 | 适用场景 |
|------|----------|--------|--------|----------|
| Redis | 定期抽样+懒删除 | 高 | 中 | 缓存、内存数据库 |
| Kafka | 按Segment删除 | 中 | 低 | 消息队列、日志系统 |
| Cassandra | 懒删除+Compaction | 中 | 中 | 分布式数据库 |
| RocksDB | CompactionFilter | 低 | 低 | KV存储引擎 |
| RobustMQ | 三层机制（推荐） | 高 | 中 | 消息中间件、存储系统 |

**核心结论：**
没有银弹，组合使用才是最佳实践！
