# Multi Raft 设计方案

> **设计原则**：单 RocksDB 实例，少量 Column Family，按 Key 前缀来区分 Raft State Machine，基于扫描自定义生成快照。

## 一、Meta Service 存储数据规划

Meta Service 需要存储三大类数据。第一类是集群相关的元数据，包括 MQTT 和 Kafka 的业务元数据以及集群配置信息，这部分数据量比较小。第二类是 MQTT 的核心业务数据，数据量会比较大，主要有 Session（客户端会话信息）、LastWill（遗嘱消息）和 RetainMessage（保留消息）这三类。第三类是 Offset 相关的数据，用来记录 Consumer 的消费进度。

基于这三类数据的特点，我们在第一阶段规划了三个独立的状态机。**Metadata State Machine** 用来处理集群元数据，数据会存在 Metadata 这个 Column Family 里。**MQTT State Machine** 负责管理 MQTT 的 Session、Last Will 和 Retain Message，这些数据统一存到 Data Column Family 中。**Offset State Machine** 专门处理消费偏移量，同样也放在 Data Column Family 里。这样做的好处是各个状态机职责清晰，互不干扰。

## 二、Multi Raft 架构

<img src="../../images/multi-raft.png" alt="Multi Raft 架构图" width="50%">

我们在 Meta Service 中设计了三个独立的 Raft State Machine，分别对应 Metadata、MQTT 和 Offset 三种业务场景，后续可以根据实际需要继续扩展。这种多状态机架构最大的价值在于隔离性和性能。不同状态机之间的网络通信和 Raft 一致性同步是完全独立的，Metadata 的操作不会阻塞 Offset 的写入，MQTT 相关的元数据处理也不会影响其他模块。当业务需求变化时，我们可以很方便地增加新的状态机来应对，整个架构具备良好的扩展性。

在存储层的设计上，所有状态机共用同一个 RocksDB 实例，但通过不同的 Column Family 来区分数据类型。**Raft Column Family** 专门存放 Raft 相关的数据，包括 `logs`（日志条目）和 `store`（状态信息，比如 vote、committed、last_purged_log_id 等）。**Data Column Family** 会用来存储大数据量的业务数据，像 Session、LastWill、RetainMessage、Offset 这些都会放在这里。**Metadata Column Family** 用来存小数据量的集群配置和元数据。这种设计既能共享 RocksDB 的资源，又能通过 Column Family 实现逻辑上的数据隔离。

## 三、存储层 Key 设计

### key 格式

从前面知道，Metadata 状态机管理的业务数据存储在RocksDB的metadata column family 中，而mqtt和offset状态机管理的业务数据存储在data column family中。因此就需要考虑如何方便构建和恢复快照。

从上面知道，metadata column family是专门用来存储Metadata 状态机管理的业务数据的，所以key格式没有强制规定，按业务规划的来。目前主要根据协议和功能划分，比如 `/mqtt/acl/{key}`、 `/kafka/cluster/{key}` 。

而data column family 就需要区分数据是mqtt状态机还是offset 状态机管理的数据。所以 mqtt 状态机管理的数据，都是用/mqtt开头，比如 `/mqtt/session/{client_id}` 存储会话信息，`/mqtt/retain/{topic}` 存储保留消息。offset 状态机管理的，都死后用/offset开头，比如：`/offset/{group_id}/{topic}/{partition}` 存储消费偏移量。这样就方便生成快照时扫描所有业务数据。

最后Raft column family专门用来存储三个Raft 状态机的元数据，比如commit、last log id等等。这些数据是根据不同状态机来区分的。比如metadata 状态机，则用/metadata开头，mqtt 状态机，则用/mqtt 开头，offset 状态机则用/mqtt开头。

### key 编码
为了优化读写性能和数据的存储大小，我们设计了一套基于哈希和前缀的 Key 编码方案。每个状态机通过 FNV-1a 哈希算法对机器名进行哈希，得到一个 8 字节的标识符，然后结合不同的前缀和数据类型来构造 Key。这种设计既能保证不同状态机的数据相互隔离，又能支持高效的范围扫描。

Raft 相关的数据使用两种 Key 格式。状态 Key 的格式是 `[前缀:1字节][机器哈希:8字节][状态类型:1字节]`，总共 10 字节，用来存储 vote、committed、last_purged_log_id 等状态信息。日志 Key 的格式是 `[前缀:1字节][机器哈希:8字节][日志索引:8字节]`，总共 17 字节，其中日志索引使用大端序编码，这样能确保字典序排列和数字大小的顺序一致，便于进行范围查询。

在性能优化方面，我们使用 `WriteBatch` 来批量写入 Raft 日志，避免频繁的磁盘 I/O。数据序列化统一使用 `bincode`，它在 Rust 生态中性能很好。网络层还会复用序列化缓冲区，减少内存分配的开销。


## 四、快照机制

快照是 Raft 协议中用来压缩日志的重要机制。当日志积累到一定程度时，我们会对状态机的当前数据生成快照，然后就可以安全地清理掉已经应用过的旧日志。每个状态机独立生成和管理自己的快照，互不干扰。

### 4.1 快照生成

快照生成是一个异步过程，不会阻塞正常的读写请求。我们会先获取 RocksDB 的一致性快照视图，然后根据不同状态机的特点来扫描数据。Metadata 状态机会扫描整个 Metadata Column Family，因为数据量不大。MQTT 和 Offset 状态机则采用前缀扫描的方式，分别扫描以 `/mqtt/` 和 `/offset/` 开头的 Key，这样可以精确地只导出属于该状态机的数据。

扫描到的数据会按照 `[key长度:4字节][key内容][value长度:4字节][value内容]` 的格式序列化，然后使用 zstd 算法压缩写入文件。快照文件命名为 `{状态机名}-{纳秒时间戳}.bin`，同时会生成一个 `.meta` 文件来存储快照的元信息，包括 last_log_id 和集群成员关系等。生成完成后，系统会记录 last_snapshot_id，并异步清理旧快照，默认保留最近 5 个版本。

### 4.2 快照恢复

节点启动或者从其他节点同步数据时，需要恢复快照。恢复过程会读取 last_snapshot_id 找到最新的快照文件，然后解压 zstd 格式的数据，按照生成时的格式反向解析出 key-value 对。为了提高写入效率，我们采用批量写入的方式，每读取 1000 条记录就提交一次到 RocksDB，这样既保证了恢复速度，又避免了内存占用过大。

恢复过程是在后台异步进行的，这样不会阻塞 Raft 状态机的初始化。如果恢复过程中遇到错误，会记录日志并继续尝试，确保系统的可用性。每个状态机的快照恢复是独立的，Metadata 恢复到 Metadata CF，MQTT 和 Offset 恢复到 Data CF 的对应前缀下。

### 4.3 快照管理

快照文件存储在配置的数据目录下的 `snapshot` 子目录中。每个状态机的快照文件通过文件名前缀来区分，比如 `metadata-1234567890.bin`、`mqtt-1234567890.bin`。系统会自动管理快照的生命周期，当快照数量超过 5 个时，会自动删除最旧的快照及其对应的 meta 文件。这个清理过程是在快照生成完成后异步触发的，不影响主流程的性能。
