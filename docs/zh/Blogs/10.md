# Meta Service 系统架构

Meta Service 是RobustMQ 内置的元数据存储组件。它类似于Zookeeper 对Kafka、NameServer 对RocketMQ的作用。它负责RobustMQ集群的协调、节点发现、元数据存储、以及一些KV型业务数据的存储。

从功能来看，Meta Service 包含下面几个方面的工作：
1. 集群协调：负责集群组建，节点发现、探活，节点间数据分发等集群协调工作。
2. 元数据存储：负责集群相关、消息队列各种协议相关的原数据的持久化存储。比如Broker信息、Topic信息、连接器信息等等。
3. KV型业务数据存储：在消息队列运行中，会有一些运行相关的数据需要持久化存储，比如MQTT 的Session数据、保留消息、遗嘱消息数据。因为这些数据的规模是可预见的，所以Meta Service也会负责这部分数据的存储。
4. 控制器： 负责集群间的一些调度工作。比如节点故障切换、Connector 任务调度等等。

所以，Meta Service 对应RobustMQ 它承担了很多工作。而比如Zookeeper 对于Kafka 来说只负责元数据存储和集群协调。为什么这么设计呢？因为Meta Service 作为一个自研内置的组建，我们想实现一个计算、存储、协调三层完全分离的架构。

## 系统架构
![img](../../images/meta-service-arch.png)
如上图所示，从技术上看，Meta Service 它的技术核心是：GRPC + Multi Raft + RocksDB。

Meta Node之间通过 GRPC 协议通信，对外也通过GRPC 提供服务。节点间数据一致性通过 Raft 协议来保证的。为了提高Meta Service的性能，Meta Service 支持 Multi Raft 架构，也就是在Meta Service 集群内会运行多个Raft 状态机。根据不同需求，会使用不同的状态机。目前支持Metadata RaftMachine、Offset RaftMachine、Data RaftMachine 三个状态机。

Meta Service的数据是通过RocksDB来持久化存储的。也就是说当写入数据时，会通过Raft Machine分发到多个节点，节点收到数据后，会将其写入到RocksDB中。另外Raft 状态机的数据和Log也是持久化存储到RocksDB的。

当 Meta Service 启动时，会根据Raft 协议规定的流程完成集群组建，投票并选举出Leader。同时会在Metadata RaftMachine的Leader节点中启动控制器线程，来进行集群调度和管理功能（比如MQTT Connector的调度和管理）。

## 如何提高Meta Service的性能
Meta Service的核心要求是性能。业界一些元数据协调服务，比如Zookeeper、ETCD等等，都会受限于性能和内存使用量，导致性能问题。zookeeper 主要受限于单Leader架构的瓶颈，另外 ZooKeeper 将所有数据存储在内存中，也导致了数据容量的限制。同样的，ETCD 也有单 Raft 架构的瓶颈，虽然ETCD的容量比Zookeeper大，但是还是有限制。

从架构上看，Meta Service 长期会承载包括数据存储在内的很多功能。所以本质上它是一个元数据协调服务 + KV存储引擎。所以，我们从设计上就避免了单Raft和内存容量限制的问题。

我们通过Multi Raft来支持多个Raft 状态机。虽然当前只有3个状态机，但是从设计上来看，可以根据需要扩容状态机的数量。比如如果元数据过多或Offset 提交频繁，则可以将Metadata RaftMachine和Offset RaftMachine的数量从一个调整到2个或者更多，从而有多个Leader可提供服务，增加并行服务的性能。

另外，我们通过RocksDB来存储KV型数据，利用RocksDB的优势来提高读写性能。此时，我们会控制内存的使用，只有一些热数据会存储在内存中会，一些数据甚至不会存在内存，会直接读写RocksDB。此时虽然会牺牲部分性能，但是可以应对比如百万Topic，甚至千万Topic等元数据量爆炸的场景。或者在MQTT场景下，可能会有亿级的连接，此时就有亿级的Session 数据需要存储的场景，则不会占用大量内存。

此时，你就可以发现，自定义实现一个内置的元数据协调服务的好处了，可以针对消息队列的特殊场景进行优化，在性能、功能、稳定性上都可以极致做优化，这是通用的组件无法满足的。这也是现在很多组件在去Zookeeper，去ETCD的原因，也是RobustMQ 要自定义实现Meta Service的原因。

## 关于网络层
Meta Service的性能瓶颈长期来看，可能会出现在网络层。理论上 GRPC协议性能挺高，但是在一些极端场景。比如MQTT 集群重启服务，上亿的连接发起连接的时候，Broker可能会对Meta Service发起高频访问。此时是Meta Service 性能最可能有瓶颈的时候。为了应对这种场景，我们有两个思路：
1. Batch 语义
2. 替换GRPC

Batch 语义 是指在某些操作，比如创建/更新Session信息，支持Batch操作。也就是Broker 调用Meta Service时一次性允许创建/更新多个Session，从而降低Broker 调用Meta Service的次数。

长期来看如果GRPC 真的是瓶颈，则需要替换GRPC协议，换位TCP或者QUIC。不过从技术上来看，在内网稳定的数据传输链路，TCP的性能和表现比QUIC更好。而如果在一些跨分区、跨地域部署的场景，如果网络条件差的情况，QUIC的表现会比TCP好。所以，长期来看，有可能会替换GRPC。不过段时间内没这个计划。


## 总结
总结来看Meta Service 是一个通过GRPC + Multi Raft + RocksDB + Batch语义来构建的针对RobustMQ 架构高度优化、适配的元数据存储服务和KV存储引擎。
