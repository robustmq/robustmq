# 关于 RobustMQ 存储引擎的思考

RobustMQ 的定位是 ALL In One 的消息队列，在设计之初就想着它能匹配所有消息中间件能覆盖的场景。你想啊，要覆盖所有场景，关键就在于存储引擎得能承载各种不同的使用场景。所以这段时间我一直在想存储引擎应该怎么设计。

## 存算分离架构的必要性

说到存算分离，很多人会问为什么消息队列要搞存算分离。其实主要有两个原因。

先说第一个，资源弹性扩展。消息队列作为存储组件，你会发现一个很尴尬的事情，CPU 使用量很低，但存储容量需求很高。我们实际运维中经常碰到两种情况：要么存储不足但 CPU 还闲着，要么 CPU 不够了但存储空间还有一大堆。前一种情况添加机器，CPU 资源就浪费了；后一种情况扩容，存储资源又浪费了。所以说存储和计算资源得能独立扩容才行。

第二个原因是成本优化。在云原生时代，硬盘或云盘的成本其实挺高的。那怎么降低存储成本呢？答案是用对象存储。对象存储是目前业界最便宜的存储选择，而且它本身就是独立服务，天然适合存算分离架构。当然了，这个方案需要你牺牲一些延时，但换来的是成本大幅下降。

## 消息队列主要场景

RobustMQ 主要希望满足以下五类场景，也就是业界在消息队列方向能遇到的所有场景（当然因为个人的认知局限，这些场景在未来可能还会增加）：

| 场景 | 核心特性 | 典型应用 |
|------|---------|---------|
| 低延时、高吞吐 | 持久化、多副本、顺序读写 | 流式处理、大数据场景 |
| 高吞吐、低成本 | 对象存储、适当牺牲延时 | 大规模数据、成本敏感场景 |
| 百万 Topic/Partition | 共享存储文件、元数据轻量 | 数据隔离、顺序性保证 |
| 极低延时、高 QPS | 内存存储、允许丢失 | 金融场景、实时数据分发 |
| 边缘消息队列 | 无依赖、轻量级、高内聚 | 边缘计算、资源受限环境 |

之前提过，消息队列主要分为消息和流两个方向，在开源社区消息方向的主要产品是RocketMQ、RabbitMQ，流方向的产品则很多，代表是Kafka和Pulsar，以及相关Kafka方向的替代产品，如Redpanda，AutoMQ、Apache IGGY。另外主打极低延时，高QPS的内存级消息队列，代表产品是：Nats。也就是说上面这五类场景基本代表了这三个方向。

## 详细场景分析

### 场景一：低延时、高吞吐场景

这是 Kafka 的主场，也算是行业标准了。Pulsar 和 Redpanda 都在这个方向上跟 Kafka 竞争。说白了就是数据持久化、多副本、可靠存储这一套，一个 Partition 对应一个文件，Append Only 顺序写、顺序读，再配合零拷贝技术把读写性能拉满。适用场景就是流式处理和大数据场景，这块 Kafka 确实是首选。

### 场景二：高吞吐、低成本场景

这其实是场景一的进阶版。业务发展到一定规模，数据量持续增长，你会发现存储和网络成本开始成为大头了。这时候怎么办呢？引入对象存储，把大量数据扔到对象存储上去，牺牲一点延时换来成本的大幅降低，这笔账还是很划算的。

### 场景三：百万 Topic/Partition 场景

有些业务需要大量分区来隔离数据，同时还得保证数据的顺序性。那问题来了，Kafka 那套一个 Partition 一个文件的模型，小文件一多元数据压力就扛不住了，根本满足不了这个场景。RocketMQ 就聪明一点，多个 MessageQueue 共享一个文件，这样能支持百万级分区，小文件少，元数据压力也小。但天下没有免费的午餐，代价就是没法顺序读了，吞吐量会下降。所以你看，这就是个权衡，分区数量和读写性能你只能选一个。

### 场景四：极低延时、高 QPS 场景

金融领域就是典型了，端到端延时得控制在 1ms 以内，当然允许丢消息。这种场景下怎么做呢？直接内存存储分发，不持久化，彻底规避磁盘 I/O 的延时。说实话，这个场景对编程语言的性能要求特别高，Java 受限于语言本身的性能天花板，很难做到这个级别。你看 Nats 走的就是内存分发、极低延时、极高 QPS 这条路，Apache IGGY 则是用文件存储 + 零拷贝 + MMAP 的方案。

### 场景五：边缘消息队列

边缘场景就更特殊了，核心诉求是无依赖、轻量级、高内聚。想想看，边缘设备资源有限，你得占用资源少、能稳定跑起来，不能依赖外部组件，只能用内存或本地文件，存储模型还得简单。在极端资源受限的环境（比如连 Java 虚拟机都跑不起来），编译型语言的优势就特别明显了。

这五个方向影响最大的是底层的存储模型，也就是说数据是怎么存储的。是存储内存，还是其他介质。是一个Partition对应一个File，还是多个Partition对应一个File。都会极大的影响读写性能。

## 存储模型对比

因为消息队列的存储特性都是 Append Only（只追加），但技术实现上其实就两种路子。

第一种是一个 Partition 一个文件，Kafka 就是这么干的。好处很明显，读写性能极高，顺序读写嘛。但坏处也很明显，小文件一多就麻烦了，硬盘性能受影响，元数据压力也大。所以这种模型适合低延时高吞吐的场景，但你别整太多 Partition。

第二种是多个 Partition 共享文件，RocketMQ 走的这条路。优点是能支持百万级分区，小文件少，元数据压力也小。代价呢？没法顺序读了，读性能会下降。这种模型适合那种需要海量 Topic/Partition 来做数据隔离的场景。

你看，这两种模型的核心就是在读写性能和分区数量支持之间做取舍。甚至延伸到其他比如关系数据库，时序数据库，其核心也是底层数据结构和索引构建方式的不同。越简单直接的读写方式，性能就越高，越复杂的，性能就越低。总结就是：功能越简单，性能越高，功能越复杂，性能越低。这是个取舍的过程。而为了解决这个取舍问题，我们将思路变为了选择。也就是说插件化存储。

## 插件化存储
说了这么多场景，你会发现一个问题：单一存储模型或单一存储引擎根本没法满足所有需求。那怎么办呢？RobustMQ 给的思路是插件化存储方案，主要包含下面三个点：

1. 支持两种存储模型（单文件和共享文件）
2. 多种存储引擎（Memory、RocksDB、Journal Engine、MySQL、对象存储等）
3. 副本策略也可以灵活配置。
   
通过这种架构，RobustMQ 就能适配从边缘到云端、从低延时到低成本的各种场景需求了。

所以在如下架构图中，我们就需要满足上面所说的三个特点，用灵活可配置的不同存储引擎、副本策略、存储模型来适配各种场景。
![img](../../images/storage-adapter.png)


## 关于副本的重新思考

这里其实有一个问题，本地存储 Memory 和 File 单机存储，没有可靠性，单机挂了不会丢数据吗？会丢数据的话，还有实际的用处吗？

先来聊一下副本的问题。传统观点认为消息队列必须要副本，否则数据丢了怎么办。但仔细想想，这个结论其实不绝对，关键还是要看业务特性。

很多场景里，消息队列主要是用来做实时数据分发的，业务上是允许数据丢失的。举几个例子你就明白了。比如股票最新价格，后面的数据会覆盖前面的，历史价格丢了对业务没影响。再比如传感器最新温度，你只关心最新读数，历史数据丢了也无所谓。还有实时监控数据，本来就是主打极低延时和高 QPS 的，允许少量丢失。

你看 Nats 的核心竞争力就是内存分发、极低延时、极高 QPS，人家压根不搞副本。Apache IGGY 主打极低延时和高吞吐，用的是文件存储 + 零拷贝 + MMAP 方案。

另外在 MQTT 场景中，默认也是不持久化数据的。接收到数据时，如果没有订阅，会直接丢弃消息。MQTT 通过Connector 将数据导入到下游的存储引擎中，Connector 也是一种订阅。

还有一个场景是云原生架构。底层云盘本身就提供多副本存储，已经能保证不丢数据了。这时候应用层如果只有单副本，影响其实就是该节点上的分区暂时不可用，但数据不会丢。在某些业务场景下，这是完全可以接受的。

但是毋庸置疑，在很多场景是需要副本的。所以是否需要副本是看场景的。单种存储模型和副本模型是很难满足所有场景的。

## 语言性能的影响

这里插一句，我们对 Redpanda 和 Kafka 做了压测对比，发现一个很有意思的现象。在相同硬件环境下，Redpanda 的延时表现明显优于 Kafka。这说明什么呢？在架构和性能优化都做到位的前提下，编程语言本身的性能差异确实会产生显著影响。Rust/C++ 这些编译型语言，在性能敏感的场景中相比 Java 有天然优势。

## 总结和展望

本文只是聊了我们在存储层的一些思考，没涉及到是具体怎么实现。说实话，具体落地没想的特别清晰。但是思路和技术方向就是上面这些。

接下来，我们的思路是：先匹配MQTT场景实现存储，然后一步一步完善具体的落地整体的存储模型。

最后聊聊未来的想法。业界已经有成熟的日志解决方案了，像 ELK（Elasticsearch + Logstash + Kibana）和阿里云 SLS（日志服务）都做得挺好。RobustMQ 长期有个设想，就是能不能适配一些简单的检索聚合场景，提供一个轻量级的 ELK 替代方案。当然了，这还只是个想法，具体怎么做还得再琢磨琢磨。
