# 压测报告

本文记录 RobustMQ 各组件的压测结果，便于回归对比和性能跟踪。

---

## 1. Meta Service

### 1.1 单节点 Raft 写入（placement-create-session）

#### 环境

| 项目 | 值 |
|------|-----|
| 机器 | MacBook Pro（Apple Silicon） |
| 部署模式 | 单节点（Meta + Broker 同进程） |
| 编译模式 | debug（`cargo run`） |
| 存储引擎 | RocksDB |

#### 命令

```bash
cargo run --package cmd --bin cli-bench meta placement-create-session \
  --host 127.0.0.1 \
  --port 1228 \
  --count 1000000 \
  --concurrency 5000 \
  --timeout-ms 3000 \
  --output table
```

#### 核心指标

| 指标 | 值 |
|------|-----|
| 总请求 | 1,000,000 |
| 成功率 | 100%（0 失败、0 超时） |
| 耗时 | 59 秒 |
| 平均 QPS | 16,949 |
| 峰值 QPS | 22,879 |
| 平均延迟 | 288.5 ms |
| P50 | 276.5 ms |
| P95 | 396.5 ms |
| P99 | 565.8 ms |
| 最大延迟 | 923.1 ms |

#### 延迟分布

- P95 / P50 = 1.43x — 分布紧凑
- P99 / P50 = 2.05x — 无严重长尾
- Max / P99 = 1.63x — 无极端离群值

#### 吞吐时间线

```
sec | ops/s  | total     | success   | failed | timeout
  1 | 19157  |    19157  |    19157  |      0 |       0
  2 | 18519  |    37676  |    37676  |      0 |       0
  3 | 20657  |    58333  |    58333  |      0 |       0
  4 | 22879  |    81212  |    81212  |      0 |       0
  5 | 10047  |    91259  |    91259  |      0 |       0
  6 | 14954  |   106213  |   106213  |      0 |       0
  7 |  5296  |   111509  |   111509  |      0 |       0
  8 | 14414  |   125923  |   125923  |      0 |       0
  9 | 14895  |   140818  |   140818  |      0 |       0
 10 | 19355  |   160173  |   160173  |      0 |       0
 11 | 14659  |   174832  |   174832  |      0 |       0
 12 | 19602  |   194434  |   194434  |      0 |       0
 13 | 19761  |   214195  |   214195  |      0 |       0
 14 | 19482  |   233677  |   233677  |      0 |       0
 15 | 19802  |   253479  |   253479  |      0 |       0
 16 | 19832  |   273311  |   273311  |      0 |       0
 17 | 19573  |   292884  |   292884  |      0 |       0
 18 | 19678  |   312562  |   312562  |      0 |       0
 19 | 19729  |   332291  |   332291  |      0 |       0
 20 | 20098  |   352389  |   352389  |      0 |       0
 21 | 19679  |   372068  |   372068  |      0 |       0
 22 | 19668  |   391736  |   391736  |      0 |       0
 23 | 20019  |   411755  |   411755  |      0 |       0
 24 | 20032  |   431787  |   431787  |      0 |       0
 25 | 14735  |   446522  |   446522  |      0 |       0
 26 | 19887  |   466409  |   466409  |      0 |       0
 27 | 20070  |   486479  |   486479  |      0 |       0
 28 | 19740  |   506219  |   506219  |      0 |       0
 29 | 19685  |   525904  |   525904  |      0 |       0
 30 | 16554  |   542458  |   542458  |      0 |       0
 31 | 18112  |   560570  |   560570  |      0 |       0
 32 | 19745  |   580315  |   580315  |      0 |       0
 33 | 15044  |   595359  |   595359  |      0 |       0
 34 | 19833  |   615192  |   615192  |      0 |       0
 35 | 15224  |   630416  |   630416  |      0 |       0
 36 | 20189  |   650605  |   650605  |      0 |       0
 37 |  9735  |   660340  |   660340  |      0 |       0
 38 | 14566  |   674906  |   674906  |      0 |       0
 39 | 15277  |   690183  |   690183  |      0 |       0
 40 | 10731  |   700914  |   700914  |      0 |       0
 41 | 18800  |   719714  |   719714  |      0 |       0
 42 | 14732  |   734446  |   734446  |      0 |       0
 43 | 19721  |   754167  |   754167  |      0 |       0
 44 | 14581  |   768748  |   768748  |      0 |       0
 45 | 15069  |   783817  |   783817  |      0 |       0
 46 | 14746  |   798563  |   798563  |      0 |       0
 47 | 18174  |   816737  |   816737  |      0 |       0
 48 | 16045  |   832782  |   832782  |      0 |       0
 49 | 14892  |   847674  |   847674  |      0 |       0
 50 | 15022  |   862696  |   862696  |      0 |       0
 51 | 14609  |   877305  |   877305  |      0 |       0
 52 | 14695  |   892000  |   892000  |      0 |       0
 53 | 19546  |   911546  |   911546  |      0 |       0
 54 | 15179  |   926725  |   926725  |      0 |       0
 55 |  9668  |   936393  |   936393  |      0 |       0
 56 | 14922  |   951315  |   951315  |      0 |       0
 57 | 14696  |   966011  |   966011  |      0 |       0
 58 | 19623  |   985634  |   985634  |      0 |       0
 59 | 14366  | 1000000   | 1000000  |      0 |       0
```

#### 分析

观察时间线，有明显的**周期性波动**：

- **高峰段**：19k-22k ops/s（秒 1-4, 10, 12-24, 26-29, 32, 34, 36, 43, 53, 58）
- **低谷段**：5k-10k ops/s（秒 5, 7, 37, 40, 55）
- **中间段**：14k-16k ops/s（其余大部分时间）

这种锯齿形波动的典型原因是 **Raft 日志落盘 + RocksDB compaction**。RocksDB 在后台做 compaction 时会短暂抢占磁盘 I/O，导致 Raft apply 变慢，QPS 掉到 5k-10k，完成后立即回升到 19k+。

以 5000 并发、平均 288ms 延迟计算，理论吞吐上限 = 5000 / 0.288 ≈ 17,361 ops/s，与实际 16,949 吻合，说明并发数是当前限制因素而非服务端瓶颈。

> **注意**：本次使用 debug 编译（`cargo run`），release 模式下性能通常可提升 2-5 倍。

### 1.2 单节点 Raft 读取（placement-list-session）

#### 环境

同 1.1（紧接写入压测后运行，RocksDB 中已有 100 万条 Session 数据）。

#### 命令

```bash
cargo run --package cmd --bin cli-bench meta placement-list-session \
  --host 127.0.0.1 \
  --port 1228 \
  --count 1000000 \
  --concurrency 5000 \
  --timeout-ms 3000 \
  --output table
```

> Bench 启动时自动创建 1 条 Session，然后对同一个 `client_id` 发起 100 万次 `ListSession` 请求。

#### 核心指标

| 指标 | 值 |
|------|-----|
| 总请求 | 1,000,000 |
| 成功率 | 100%（0 失败、0 超时） |
| 返回记录 | 1,000,000（每次返回 1 条） |
| 耗时 | 11 秒 |
| 平均 QPS | 90,909 |
| 峰值 QPS | 102,714 |
| 平均延迟 | 45.6 ms |
| P50 | 43.0 ms |
| P95 | 82.3 ms |
| P99 | 102.3 ms |
| 最大延迟 | 198.7 ms |

#### 延迟分布

- P95 / P50 = 1.91x — 分布良好
- P99 / P50 = 2.38x — 无严重长尾
- Max / P99 = 1.94x — 无极端离群值

#### 吞吐时间线

```
sec | ops/s   | total     | success   | failed | timeout | received
  1 |  92529  |    92529  |    92529  |      0 |       0 |    92529
  2 | 100804  |   193333  |   193333  |      0 |       0 |   193333
  3 | 102714  |   296047  |   296047  |      0 |       0 |   296047
  4 |  95409  |   391456  |   391456  |      0 |       0 |   391456
  5 |  92831  |   484287  |   484287  |      0 |       0 |   484287
  6 |  89426  |   573713  |   573713  |      0 |       0 |   573713
  7 |  93254  |   666967  |   666967  |      0 |       0 |   666967
  8 |  90311  |   757278  |   757278  |      0 |       0 |   757278
  9 |  93433  |   850711  |   850711  |      0 |       0 |   850711
 10 |  99942  |   950653  |   950653  |      0 |       0 |   950653
 11 |  49347  |  1000000  |  1000000  |      0 |       0 |  1000000
```

#### 分析

读取吞吐非常稳定，全程维持在 **89k-103k ops/s**，没有写入场景中的锯齿形波动。这是因为：

- **读取不经过 Raft 共识**：`ListSession` 直接从 RocksDB 读取，无需日志复制和多数派确认。
- **无 WAL 写入开销**：读操作不触发 RocksDB 的 Write-Ahead Log，不受 compaction 影响。
- **延迟大幅降低**：平均 45.6ms vs 写入的 288.5ms，读取延迟仅为写入的 **1/6**。

#### 读写对比

| 指标 | 写入（create-session） | 读取（list-session） | 读/写倍数 |
|------|----------------------|---------------------|----------|
| 平均 QPS | 16,949 | 90,909 | **5.4x** |
| 峰值 QPS | 22,879 | 102,714 | **4.5x** |
| 平均延迟 | 288.5 ms | 45.6 ms | **6.3x 更快** |
| P99 延迟 | 565.8 ms | 102.3 ms | **5.5x 更快** |
| 吞吐波动 | 明显（5k-22k） | 平稳（89k-103k） | — |

> **注意**：本次使用 debug 编译（`cargo run`），release 模式下性能通常可提升 2-5 倍。

---

## 2. MQTT Broker

> 待补充。
